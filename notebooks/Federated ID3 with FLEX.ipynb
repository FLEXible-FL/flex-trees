{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated ID3 using FLEX library. \n",
    "\n",
    "\n",
    "In this notebook we show how to use the *Federated ID3* model, from the [paper](https://arxiv.org/pdf/2007.10987.pdf).\n",
    "\n",
    "First we do all the imports needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from flex.data import FedDataDistribution, FedDatasetConfig\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "from flextrees.datasets.tabular_datasets import nursery\n",
    "from flextrees.pool.primitives_fedid3 import (\n",
    "    init_server_model_id3, \n",
    "    deploy_server_config_id3,\n",
    "    deploy_server_model_id3,\n",
    "    build_id3,\n",
    "    set_aggregated_id3,\n",
    "    evaluate_id3_model,\n",
    "    evaluate_global_model_clients,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data using FLEX.\n",
    "\n",
    "In this tutorial we are going to use the **nursery** database. We can use it by importing the dataset using the flextrees library. In this model the server needs to know the unique values from all the features of the dataset that is been used, so after loading it, we have to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, features_names = nursery(ret_feature_names=True, categorical=True)\n",
    "unique_values = []\n",
    "for i, val in enumerate(features_names[:-1]):\n",
    "    unique_values_feature = list(set(train_data.X_data.to_numpy()[:,i]))\n",
    "    unique_values.append(unique_values_feature)\n",
    "n_clients = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federating the data using FLEX\n",
    "\n",
    "Once the data is loaded, we have to federate it. To do so we use the FLEX library. We show to ways of federating the data, using a iid distribution or a non-idd distribution. For the IID distribution we can just use the the `ìid_distribution` function from FedDataDistribution. If we are using a non-iid distribution, we have to use a custom configuration and, in this case, we just set the seed, the number of clients, and we can set manually the weights by creating them randomly or whatever the user wants. For more information, go to the FLEX library notebooks, and take a look at the notebook *Federating data with FLEXible*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'iid'\n",
    "\n",
    "if dist == 'iid':\n",
    "    federated_data = FedDataDistribution.iid_distribution(centralized_data=train_data,\n",
    "                                                        n_nodes=n_clients)\n",
    "else:\n",
    "    weights = np.random.dirichlet(np.repeat(1, n_clients), 1)[0] # To generate random weights (Full Non-IID)\n",
    "    config_nidd = FedDatasetConfig(seed=0, n_nodes=n_clients, \n",
    "                                replacement=False, weights=weights)\n",
    "\n",
    "    federated_data = FedDataDistribution.from_config(centralized_data=train_data,\n",
    "                                                        config=config_nidd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the federated architecture\n",
    "\n",
    "When creating the federated architecture, we use `FlexPool`. As we're running a client-server architecture, we use the function `client_server_architecture`. We need to give to this function the dimension of the dataset for creating the LSH functions in order of creating the planes to hash all the data from the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = FlexPool.client_server_pool(federated_data, init_server_model_id3,\n",
    "                                        dataset_features = features_names)\n",
    "\n",
    "clients = pool.clients\n",
    "aggregator = pool.aggregators\n",
    "server = pool.servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we set the configuration for all the clients for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy clients config\n",
    "pool.servers.map(func=deploy_server_config_id3, dst_pool=pool.clients)\n",
    "root_ = None\n",
    "value_features = {\n",
    "    feature: unique_values[i]\n",
    "    for i, feature in enumerate(features_names[:-1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "As the model is built recursively, we've built a primitive function, `build_id3` that builds the tree. This function only needs to recieve the initialized root as *None*, the maximum depth of the problem, that is defined to *n_features/2* in the paper, and the rest of parameters needed to build the tree. Note that we also need to give the *pool* to the function, so we can build the tree in a federated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the ID3 tree\n",
    "root_ = build_id3(\n",
    "    node=root_,\n",
    "    depth=1,\n",
    "    available_features=features_names[:-1],\n",
    "    pool=pool,\n",
    "    max_depth=len(features_names) // 2,\n",
    "    values_features=value_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model\n",
    "\n",
    "Deploy the model across the clients so they can use it within its local data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.aggregators._models['server']['aggregated_weights'] = root_\n",
    "pool.aggregators.map(set_aggregated_id3, pool.servers)\n",
    "pool.servers.map(deploy_server_model_id3, pool.clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model at client's side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "path_to_results_folder = os.path.abspath('../')\n",
    "filename = f\"/resultados_{nursery.__name__}_clients_{n_clients}_{dist}_exec_{exec}_{datetime.now().strftime('%Y_%m_%d-%I_%M_%S_%p')}.csv\"\n",
    "filename = f\"{path_to_results_folder}/results_fedid3/{filename}\"\n",
    "pool.clients.map(evaluate_global_model_clients, filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with a global test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.servers.map(evaluate_id3_model, test_data=test_data, filename=filename, etime=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flextrees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
