{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Federated Gradient Boosting Decision Trees using FLEX library. \n",
    "\n",
    "\n",
    "In this notebook we show how to use the *Practical Federated Gradient Boosting Decision Trees* model, from the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/5895).\n",
    "\n",
    "Note: the preprocessing stage takes a lot of time to finish, so \n",
    "\n",
    "First we do all the imports needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from flex.data import FedDataDistribution, FedDatasetConfig, one_hot_encoding\n",
    "from flex.pool import FlexPool\n",
    "\n",
    "from flextrees.datasets.tabular_datasets import adult\n",
    "\n",
    "from flextrees.pool import (\n",
    "    init_server_model_gbdt,\n",
    "    init_hash_tables,\n",
    "    compute_hash_values,\n",
    "    deploy_server_config_gbdt,\n",
    "    evaluate_global_model,\n",
    "    evaluate_global_model_clients_gbdt,\n",
    "    train_n_estimators,\n",
    "    preprocessing_stage,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data using FLEX.\n",
    "\n",
    "In this tutorial we are going to use the adult database. We can use it by importing the dataset using the flextrees library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = adult(ret_feature_names=False, categorical=False)\n",
    "n_labels = len(np.unique(train_data.y_data.to_numpy())) # We need the number of total labels for the softmax.\n",
    "dataset_dim = train_data.to_numpy()[0].shape[1] # We need the dimension to create the LSH hyper planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federating the data using FLEX\n",
    "\n",
    "Once the data is loaded, we have to federate it. To do so we use the FLEX library. We show to ways of federating the data, using a iid distribution or a non-idd distribution. For the IID distribution we can just use the the `ìid_distribution` function from FedDataDistribution. If we are using a non-iid distribution, we have to use a custom configuration and, in this case, we just set the seed, the number of clients, and we can set manually the weights by creating them randomly or whatever the user wants. For more information, go to the FLEX library notebooks, and take a look at the notebook *Federating data with FLEXible*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'iid'\n",
    "\n",
    "if dist == 'iid':\n",
    "    federated_data = FedDataDistribution.iid_distribution(centralized_data=train_data,\n",
    "                                                        n_nodes=n_clients)\n",
    "else:\n",
    "    config_nidd = FedDatasetConfig(seed=0, n_nodes=n_clients, replacement=False)\n",
    "\n",
    "    federated_data = FedDataDistribution.from_config(centralized_data=train_data,\n",
    "                                                        config=config_nidd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using the model, we will need to do a little preprocess to the data, and this is to ``one hot encode`` the labels. After federating the data, we can use the `apply` function from the `FedDataset` to apply the selected function to all the data that is federated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the labels for using softmax\n",
    "federated_data.apply(one_hot_encoding, n_labels=n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the federated architecture\n",
    "\n",
    "When creating the federated architecture, we use `FlexPool`. As we're running a client-server architecture, we use the function `client_server_architecture`. We need to give to this function the dimension of the dataset for creating the LSH functions in order of creating the planes to hash all the data from the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = FlexPool.client_server_architecture(federated_data, init_server_model_gbdt, dataset_dim=dataset_dim)\n",
    "clients = pool.clients\n",
    "aggregator = pool.aggregators\n",
    "server = pool.servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of estimators\n",
    "total_estimators = 10\n",
    "print(f\"Number of trees to build: {total_estimators}\")\n",
    "estimators_built = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we set the configuration for all the clients for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.map(func=deploy_server_config_gbdt, dst_pool=pool.clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's everything is set, we can begin with the code for the boosting model. First the client's have to create the hash table for its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients.map(func=init_hash_tables) # Init hash tables\n",
    "clients.map(func=compute_hash_values) # Calculate the hash tables on the clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing stage\n",
    "\n",
    "This phase use the hash tables from the clients to search the similar instances for each instance from other clients without sharing the data, just sharing the planes. This phase is complex, the pseudo-code is available in the paper, and we have put all together into one primitive function.\n",
    "\n",
    "The `preprocessing_stage` function recieves the clients, the server and the aggregator and create the global hash table with the similar instances for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_stage(clients=clients,\n",
    "                        server=server,\n",
    "                        aggregator=aggregator\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training stage\n",
    "\n",
    "The second phase is the training phase. As it's done with the preprocessing stage, we have a primitive function to train all the estimators selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_estimators(clients=clients, server=server,\n",
    "                    aggregator=aggregator, total_estimators=total_estimators,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "After the model is trained, we have to evaluate it at the server level and at client level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On server side\n",
    "server.map(evaluate_global_model, test_data=test_data)\n",
    "# On clients side\n",
    "clients.map(evaluate_global_model_clients_gbdt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flextrees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
